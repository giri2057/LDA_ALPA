# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YLGmNS1mQ97km9N6oX2EkWPAfZLxuLQZ
"""

!pip install docx

!pip install pyldavis

pip install python-docx

import os
import pandas as pd
import re
import numpy as np
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import docx
from gensim import corpora, models
from gensim.models.ldamodel import LdaModel
import string
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
from nltk.tokenize import word_tokenize

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Function to read text from a Word document
def read_word_file(file_path):
    doc = docx.Document(file_path)
    full_text = []
    for paragraph in doc.paragraphs:
        full_text.append(paragraph.text)
    return ' '.join(full_text)

# Define preprocessing function
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    return tokens

# Function to save preprocessed text to CSV
def save_to_csv(tokens, output_csv_path):
    df = pd.DataFrame({'Token': [' '.join(tokens)]})
    df.to_csv(output_csv_path, index=False)

# Main process
file_path = '/content/ALPA_2013-14AnnualReport.docx'
output_csv_path = '/content/ALPA_2013-14AnnualReport AnnualReport.csv'

# Read and preprocess the text data
text_data = read_word_file(file_path)
tokens = preprocess_text(text_data)

# Save preprocessed tokens to CSV
save_to_csv(tokens, output_csv_path)

# Load preprocessed tokens into DataFrame
df = pd.read_csv(output_csv_path)

# Convert the single string of tokens back to a list of tokens
tokens = df['Token'][0].split()

# Create dictionary and document-term matrix
dictionary = corpora.Dictionary([tokens])
doc_term_matrix = [dictionary.doc2bow(tokens)]

# Define the number of topics
num_topics = 10 # Adjust the number of topics as needed

# Run LDA model
lda_model = LdaModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=30)

# Print topics and associated words
for idx, topic in lda_model.print_topics(-1):
    print(f'Topic: {idx} \nWords: {topic}\n')

# Visualize the topics
vis = gensimvis.prepare(lda_model, doc_term_matrix, dictionary)

# Check the content of `vis` to ensure it does not contain complex numbers
import json

try:
    json.dumps(vis)
except TypeError as e:
    print(f"Error serializing vis: {e}")

# Ensure you are in a Jupyter Notebook environment for the next line to work
pyLDAvis.enable_notebook()

pyLDAvis.display(vis)

